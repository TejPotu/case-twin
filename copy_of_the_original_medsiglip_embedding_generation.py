# -*- coding: utf-8 -*-
"""Copy of the_original_medsiglip_embedding_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ktYmnjEjjN7KQyHYbg9nX4fmEjZsVD4j

# CaseTwin: Chest X-Ray Embedding Generation + GCS Upload
Generate MedSiglip-448 embeddings, upload images to GCS, and store in Qdrant

**Dataset**: PMC Chest X-rays from Google Drive  
**Model**: google/medsiglip-448  
**Storage**: Google Cloud Storage  
**Vector DB**: Qdrant Cloud (Free Tier)  
**Batch Size**: 5 cases with ALL their chest X-rays

## 1. Setup & Installation
"""

# Install required packages
!pip install -q transformers torch pillow qdrant-client tqdm google-cloud-storage

import json
import os
from pathlib import Path
from PIL import Image
import torch
from transformers import AutoModel, AutoProcessor
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from google.cloud import storage
from tqdm import tqdm
import uuid
from datetime import timedelta

print("âœ… All libraries imported successfully!")
print(f"ğŸ–¥ï¸  Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}")

"""## 2. Mount Google Drive & Setup GCS"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Authenticate with Google Cloud (for GCS access)
from google.colab import auth
auth.authenticate_user()

print("âœ… Authenticated with Google Cloud")

# âš™ï¸ CONFIGURE THESE PATHS
# Adjust base path if needed
DRIVE_BASE_PATH = "/cxr_full_dataset"

# POINTS TO THE NEW DATASET
DATASET_JSON_PATH = f"{DRIVE_BASE_PATH}/dataset_cxr_primary.json"
IMAGES_FOLDER_PATH = f"{DRIVE_BASE_PATH}/images"

# âš™ï¸ CONFIGURE GCS SETTINGS
GCS_PROJECT_ID = "project_id"  # Your GCP project ID
GCS_BUCKET_NAME = "casetwin-xrays"  # Bucket name
GCS_FOLDER_PREFIX = "chest-xrays/"  # Folder inside bucket

# Verify paths
assert os.path.exists(DATASET_JSON_PATH), f"âŒ dataset_cxr_primary.json not found at {DATASET_JSON_PATH}"
assert os.path.exists(IMAGES_FOLDER_PATH), f"âŒ images folder not found at {IMAGES_FOLDER_PATH}"

print(f"âœ… Dataset JSON: {DATASET_JSON_PATH}")
print(f"âœ… Images Folder: {IMAGES_FOLDER_PATH}")
print(f"â˜ï¸  GCS Bucket: gs://{GCS_BUCKET_NAME}/{GCS_FOLDER_PREFIX}")

# Initialize GCS client
storage_client = storage.Client(project=GCS_PROJECT_ID)

try:
    bucket = storage_client.get_bucket(GCS_BUCKET_NAME)
    print(f"âœ… Using existing bucket: {GCS_BUCKET_NAME}")
except:
    bucket = storage_client.create_bucket(GCS_BUCKET_NAME, location="US")
    print(f"âœ… Created new bucket: {GCS_BUCKET_NAME}")

print(f"ğŸ“¦ Bucket location: {bucket.location}")

"""## 3. Configure Qdrant Connection"""

# CONFIGURE QDRANT CREDENTIALS
QDRANT_URL = ""
QDRANT_API_KEY = ""  # Get from Qdrant Cloud > API Keys
COLLECTION_NAME = "chest_xrays"

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
print("âœ… Connected to Qdrant Cloud!")

"""## 4. Load MedSiglip-448 Model"""

from huggingface_hub import login
login()

MODEL_NAME = "google/medsiglip-448"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸ”„ Loading {MODEL_NAME} on {device}...")
processor = AutoProcessor.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device)
model.eval()
print("âœ… Model loaded!")

"""## 5. Create Qdrant Collection"""

# Get embedding dimension
test_image = Image.new('RGB', (448, 448), color='white')
test_inputs = processor(images=test_image, return_tensors="pt").to(device)

with torch.no_grad():
    test_outputs = model.get_image_features(**test_inputs)
    # FIX: Use pooler_output instead of direct .shape
    EMBEDDING_DIM = test_outputs.pooler_output.shape[-1]

print(f"ğŸ“ Embedding dimension: {EMBEDDING_DIM}")

# Create collection
try:
    qdrant_client.delete_collection(collection_name=COLLECTION_NAME)
    print(f"ğŸ—‘ï¸  Deleted existing collection")
except:
    pass

qdrant_client.create_collection(collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)
)
print(f"âœ… Created collection '{COLLECTION_NAME}'")

"""## 6. Helper Functions"""

def upload_to_gcs(local_path, gcs_filename):
    """Upload image to GCS and return public URL"""
    try:
        blob_name = f"{GCS_FOLDER_PREFIX}{gcs_filename}"
        blob = bucket.blob(blob_name)
        blob.upload_from_filename(local_path)
        blob.make_public()
        return blob.public_url
    except Exception as e:
        print(f"âŒ GCS upload error: {e}")
        return None

def generate_embedding(image_path):
    image = Image.open(image_path).convert('RGB')
    inputs = processor(images=image, return_tensors="pt").to(device)

    with torch.no_grad():
        features = model.get_image_features(**inputs)
        embedding = features.pooler_output  # get the tensor
        embedding = embedding / embedding.norm(dim=-1, keepdim=True)  # normalize

    return embedding.cpu().float().numpy().squeeze().tolist()

def prepare_metadata(case, image_data, gcs_url, is_primary=True):
    """
    Store the FULL case payload in Qdrant.
    Handles both 'study' (primary) and 'related_images' (secondary).
    """
    # Root objects
    patient = case.get('patient', {})
    presentation = case.get('presentation', {})
    assessment = case.get('assessment', {})
    provenance = case.get('provenance', {})
    tags = case.get('tags', {})  # Root tags often apply to the whole case

    # Extract Image-Specific Metadata
    if is_primary:
        # Primary Image (from 'study')
        filename = image_data.get('storage_path', '').split('/')[-1]
        modality = image_data.get('modality')
        region = image_data.get('body_region')
        view = image_data.get('view_position')
        caption = image_data.get('caption')
        image_type = image_data.get('image_type', 'radiology')
        image_subtype = image_data.get('image_subtype', 'x_ray')

        # Use case-level labels for primary if specific ones aren't on the study object
        ml_labels = tags.get('ml_labels', [])
        gt_labels = tags.get('gt_labels', [])

        image_id = case.get('image_id') # Primary image ID is often at root or valid here
    else:
        # Related Image
        filename = image_data.get('local_image_path', '').split('/')[-1]
        modality = image_data.get('image_type') # e.g. 'radiology', 'pathology'
        region = image_data.get('radiology_region')
        view = image_data.get('view_position')
        caption = image_data.get('caption')
        image_type = image_data.get('image_type')
        image_subtype = image_data.get('image_subtype')

        ml_labels = image_data.get('ml_labels', [])
        gt_labels = image_data.get('gt_labels', [])

        image_id = image_data.get('image_id')

    return {
        # â”€â”€ GCS / image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "image_url":       gcs_url,
        "gcs_path":        f"gs://{GCS_BUCKET_NAME}/{GCS_FOLDER_PREFIX}{filename}",
        "filename":        filename,
        "is_primary":      is_primary,

        "image_id":        image_id,
        "case_id":         case.get('case_id'),
        "profile_id":      case.get('profile_id'),

        "caption":         caption,
        "modality":        modality,
        "image_type":      image_type,
        "image_subtype":   image_subtype,
        "radiology_region":region,
        "view_position":   view,

        "ml_labels":       ml_labels,
        "gt_labels":       gt_labels,

        # â”€â”€ Source / article â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "pmc_id":          provenance.get('pmc_id'),
        "article_title":   provenance.get('article_title'),
        "journal":         provenance.get('journal'),
        "year":            provenance.get('year'),
        "authors":         provenance.get('authors', []),
        "license":         provenance.get('license'),
        "dataset_name":    provenance.get('dataset_name'),

        # â”€â”€ Patient context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "age":             patient.get('age_years'),
        "sex":             patient.get('sex'),
        "immunocompromised": patient.get('immunocompromised'),
        "chief_complaint": presentation.get('chief_complaint'),
        "symptom_duration":presentation.get('symptom_duration'),
        "comorbidities":   patient.get('comorbidities', []),
        "medications":     patient.get('medications', []),

        # â”€â”€ Clinical & Diagnosis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "diagnosis":       assessment.get('diagnosis_primary'),
        "differential":    assessment.get('differential', []),
        "urgency":         assessment.get('urgency'),
        "infectious_concern": assessment.get('infectious_concern'),

        "summary_oneliner": case.get('summary', {}).get('one_liner'),
    }

print("âœ… Helper functions updated for New Dataset Schema")

"""## 7. Process Chest X-Rays (5 Cases with ALL Images)"""

# Configuration
MAX_CASES =  1000

print(f"ğŸ“Š Total cases in dataset: {len(dataset)}")
print(f"ğŸ”„ Processing first {MAX_CASES} cases (Primary + Related images)...\\n")

points = []
upload_log = []
processed = 0
failed = 0

# Limit to MAX_CASES
target_cases = dataset[:MAX_CASES]

for case_idx, case in enumerate(target_cases, 1):
    case_id = case.get('case_id')
    pmc_id = case.get('provenance', {}).get('pmc_id', 'Unknown')

    print(f"\\n{'='*70}")
    print(f"ğŸ“‹ Case {case_idx}/{MAX_CASES}: {pmc_id} (ID: {case_id})")
    print(f"{'='*70}")


    # 1. Process Primary Image ('study')

    study = case.get('study', {})
    if study:
        path_rel = study.get('storage_path')
        if path_rel:
            filename = path_rel.split('/')[-1]
            local_path = os.path.join(IMAGES_FOLDER_PATH, filename)

            # Fallback check if file is just in root
            if not os.path.exists(local_path):
                 local_path = os.path.join(IMAGES_FOLDER_PATH, path_rel)

            if os.path.exists(local_path):
                print(f"   [Primary] ğŸ“¤ {filename}...")
                try:
                    # Upload
                    gcs_url = upload_to_gcs(local_path, filename)
                    if gcs_url:
                        # Embed
                        embedding = generate_embedding(local_path)
                        if embedding:
                            # Payload
                            metadata = prepare_metadata(case, study, gcs_url, is_primary=True)

                            point = PointStruct(id=str(uuid.uuid4()), vector=embedding, payload=metadata)
                            points.append(point)
                            processed += 1
                            print(f"      âœ… Success (Primary)")
                        else:
                             print("      âŒ Embedding failed")
                             failed += 1
                    else:
                        print("      âŒ Upload failed")
                        failed += 1
                except Exception as e:
                    print(f"      âŒ Generat Error: {e}")
                    failed += 1
            else:
                print(f"   âš ï¸  Primary Image not found: {local_path}")
                failed += 1


    # 2. Process Related Images ('related_images')

    related_list = case.get('related_images', [])
    for r_img in related_list:
        path_rel = r_img.get('local_image_path')
        if path_rel:
            filename = path_rel.split('/')[-1]
            local_path = os.path.join(IMAGES_FOLDER_PATH, filename)

            if not os.path.exists(local_path):
                 local_path = os.path.join(IMAGES_FOLDER_PATH, path_rel)

            if os.path.exists(local_path):
                img_type = r_img.get('image_type', 'unknown')
                print(f"   [Related] ğŸ“¤ {filename} ({img_type})...")
                try:
                    gcs_url = upload_to_gcs(local_path, filename)
                    if gcs_url:
                        embedding = generate_embedding(local_path)
                        if embedding:
                            metadata = prepare_metadata(case, r_img, gcs_url, is_primary=False)

                            point = PointStruct(id=str(uuid.uuid4()), vector=embedding, payload=metadata)
                            points.append(point)
                            processed += 1
                            print(f"      âœ… Success (Related)")
                except Exception as e:
                    print(f"      âŒ Error: {e}")
                    failed += 1
            else:
                print(f"   âš ï¸  Related Image not found: {local_path}")

print(f"\\n\\nğŸ“Š Processing complete!")
print(f"   âœ… Successful points generated: {processed}")
print(f"   âŒ Failed: {failed}")

# Upload to Qdrant
if points:
    print(f"\nâ¬†ï¸  Uploading {len(points)} points to Qdrant...")
    qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)
    print(f"âœ… Successfully uploaded!\n")

    print("ğŸ“‹ Upload Summary:")
    for log in upload_log:
        print(f"   â€¢ {log['pmc_id']}: {log['filename'][:50]}")
        if log.get('age') or log.get('gender'):
            print(f"     Patient: {log.get('age')}y/{log.get('gender')}")

"""## 8. Verify Upload"""

# Collection stats
info = qdrant_client.get_collection(COLLECTION_NAME)
print("ğŸ“Š Collection Status:")
print(f"   Name: {COLLECTION_NAME}")
print(f"   Points: {info.points_count}")
print(f"   Vector size: {info.config.params.vectors.size}")

# Sample point
if points:
    sample = qdrant_client.retrieve(
        collection_name=COLLECTION_NAME,
        ids=[points[0].id]
    )[0]

    print("\nğŸ” Sample Point:")
    print(f"   ID: {sample.id}")
    print(f"\n   Metadata:")
    print(json.dumps(sample.payload, indent=4))